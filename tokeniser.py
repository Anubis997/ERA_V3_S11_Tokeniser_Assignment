# -*- coding: utf-8 -*-
"""Tokeniser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uNERVZrTF9I4SxVLqZkpIX4Oi0tKrDLe
"""

# prompt: write code for accessing kaggle  in a secured manner

import torch
import os

# Replace with your actual Kaggle credentials
os.environ['KAGGLE_USERNAME'] = "ajaykumarnadimpalli"
os.environ['KAGGLE_KEY'] = input()

import kagglehub

# Download latest version
path = kagglehub.dataset_download("disisbig/hindi-wikipedia-articles-55k")


print("Path to dataset files:", path)

# prompt: I have huge amount of text files in "/root/.cache/kagglehub/datasets/disisbig/hindi-wikipedia-articles-55k/versions/1/train/train" and in "/root/.cache/kagglehub/datasets/disisbig/hindi-wikipedia-articles-55k/versions/1/valid/valid"  folders as well. Create a list which appends all the text in those text files

import os

def read_text_files(directory):
  """Reads all text files in a directory and returns a list of their contents."""
  text_data = []
  for filename in os.listdir(directory):
    if filename.endswith(".txt"):
      filepath = os.path.join(directory, filename)
      try:
        with open(filepath, 'r', encoding='utf-8') as f: # Specify encoding
          text_data.append(f.read())
      except Exception as e:
        print(f"Error reading file {filename}: {e}")
  return text_data

train_dir = "/root/.cache/kagglehub/datasets/disisbig/hindi-wikipedia-articles-55k/versions/1/train/train"
valid_dir = "/root/.cache/kagglehub/datasets/disisbig/hindi-wikipedia-articles-55k/versions/1/valid/valid"

all_text_data = []

all_text_data.extend(read_text_files(train_dir))
all_text_data.extend(read_text_files(valid_dir))

print(f"Total number of files processed: {len(all_text_data)}")
# Now all_text_data contains the text content of all .txt files from both directories

# prompt: combine pattern = r"[\w\'-]+|[.,!?;:()\[\]{}]"
#     tokens = re.findall(pattern, text) and this    pattern = r"[^\u0900-\u097F\s]+"  # Matches anything NOT within the Hindi Unicode range
#     cleaned_text = re.sub(pattern, "", text)

import re

def clean_text(text):
    pattern = r"[\w\'-]+|[.,!?;:()\[\]{}]"
    tokens = re.findall(pattern, text)

    pattern = r"[^\u0900-\u097F\s]+"  # Matches anything NOT within the Hindi Unicode range
    cleaned_text = []
    for token in tokens:
      cleaned_token = re.sub(pattern, "", token)
      if cleaned_token: #check if string is empty after removing non-hindi characters
        cleaned_text.append(cleaned_token)
    return cleaned_text

cleaned_data = []
for text in all_text_data:
    cleaned_data.extend(clean_text(text)) # Use extend instead of append

# prompt: shuffle this list and select first 200000 bag_of_encoded_words=[x.encode("utf-8") for x in cleaned_data][:2000000]

import random
import json

# Assuming cleaned_data is defined as in your provided code
# ... (your existing code) ...

bag_of_encoded_words = [x.encode("utf-8") for x in cleaned_data][:10000]

# Shuffle the list
random.shuffle(bag_of_encoded_words)

import collections
from typing import List, Tuple, Dict

def get_stats(data: List[bytes]) -> Dict[Tuple[bytes, bytes], int]:
    """Count frequency of consecutive character pairs in data."""
    counts = collections.Counter(zip(data, data[1:]))
    return counts

def merge(data: List[bytes], pair: Tuple[bytes, bytes], new_token: bytes) -> List[bytes]:
    """Merge occurrences of a character pair into a new token."""
    new_data = []
    i = 0
    while i < len(data):
        if i < len(data) - 1 and data[i] == pair[0] and data[i + 1] == pair[1]:
            new_data.append(new_token)
            i += 2
        else:
            new_data.append(data[i])
            i += 1
    return new_data

def train_bpe(text: List[bytes], num_merges: int):
    """Train Byte Pair Encoding (BPE) on a list of UTF-8 characters."""
    data = text  # Treat characters (not bytes) as tokens
    vocab = {tuple([char]): char for char in set(data)}  # Initial vocab
    next_token_id = len(vocab)

    merges = {}
    current_data = data.copy()
    vocab_size = len(vocab)
    total_tokens = len(data)

    for _ in range(num_merges):
        counts = get_stats(current_data)
        if not counts:
            break
        pair, _ = max(counts.items(), key=lambda x: x[1])
        new_token = f"_{next_token_id}".encode('utf-8')  # Unique new token # encode to bytes

        merges[pair] = new_token
        vocab[pair] = new_token
        next_token_id += 1
        vocab_size += 1
        current_data = merge(current_data, pair, new_token)

        total_tokens = len(current_data)

    return merges, vocab, current_data, total_tokens, vocab_size

def encode_text(text: str, merges: Dict[Tuple[str, str], str]):
    """Encode UTF-8 text using trained BPE merges."""
    data = list(text)
    for pair, new_token in merges.items():
        data = merge(data, pair, new_token)
    return data

def decode_text(encoded_data: List[str], vocab: Dict[Tuple[str, str], str]):
    """Decode BPE-encoded text back to original UTF-8 characters."""
    reverse_vocab = {v: k for k, v in vocab.items()}
    decoded_chars = []
    for token in encoded_data:
        if token in reverse_vocab:
            decoded_chars.extend(reverse_vocab[token])
        else:
            # Decode the token if it's bytes
            decoded_chars.append(token.decode('utf-8') if isinstance(token, bytes) else token)
    return "".join(decoded_chars)  # Convert back to string

def calculate_compression_ratio(original_data: List[bytes], encoded_data: List[bytes]) -> float:
    """Calculate compression ratio between original and encoded data."""
    original_size = len(original_data)
    encoded_size = len(encoded_data)
    compression_ratio = original_size / encoded_size if encoded_size > 0 else float('inf')
    return compression_ratio

if __name__ == "__main__":
    num_merges = 5000  # Adjust based on desired merge steps

    # Train BPE tokenizer
    merges, vocab, encoded_data, total_tokens, vocab_size = train_bpe(bag_of_encoded_words, num_merges)

   
    compression_ratio = calculate_compression_ratio(bag_of_encoded_words, encoded_data)

#Example
hindi_sentence = "जो कि पक्षपाती हैं और बेतरतीबी नतीज़ें देती हैं।"
encoded_hindi = encode_text(hindi_sentence, merges) # Encoding
decoded_hindi = decode_text(encoded_hindi, vocab)   # Decoding

print("Encoded:", encoded_hindi)
print("Decoded:", decoded_hindi)

